---
title: 'Everything you will see may well be fake'
summary: 'The consequences of AI progress on manipulation
of digital media, what we can do about it, and some experiments on detecting face swaps'
layout: post
date: 2018-01-01
permalink: /posts/2018/01/01/fake/
tags:
  - machine Learning
  - security
  - society
img-folder: 2018-01-01-fake
---

<div style="text-align: right">
written with Simone Lini
</div>

<br><br>

*TLDR;
It is becoming widely evident that, in a near future, technology will enable total manipulation of video and audio content, as well as its digital creation from scratch.
As a consequence, the meaning of evidence and truth will be critically challenged and pillars of modern society such as information, justice and democracy will go through a period of fundamental crisis and need of re-definition.
The effects will be more dramatic than the current phenomenon of "fake news", when tools for fabrication of evidence backing fictional facts becomes a commodity.
In the academic and tech circles those issues are being discussed only at a philosophical level; no clear solution is either known or argued at current time.
This post discusses two natural classes of potential technological solutions, with different pros and cons: digital signatures and machine learning based verification systems.
We also run a brief "weekend experiment" to measure the potential of the second solution to detect human faces manipulation, on the wave of [deepfakes](https://motherboard.vice.com/en_us/topic/deepfakes) face swaps on videos.
Using off-the-shelf deep learning tools, we learn to recognize face swaps and compare performance with some human testers. In the limited scope of our experiment, our model is able to classify manipulated faces that are imperceptible to humans.*

<br>

**In 1983,** at the peak of Cold War, [Stanislav Yevgrafovich Petrov](https://en.wikipedia.org/wiki/Stanislav_Petrov) was a lieutenant colonel stationed at the Serpukhov-15 bunker, close to the Soviet capital. He was monitoring the early warning system which was in charge of detecting nuclear missile launches from the United States.

<img src="/assets/posts/2018-01-01-fake/petrov.jpg" width="110%">

In September 26, the bunker systems alerted Petrov of a missile launch from Montana.
US and USSR were following the `mutual assured destruction` doctrine. If the Americans were to launch a nuclear attack, the Soviets would have retaliated with a massive counterattack, ensuring the annihilation of both countries.

Had Petrov alerted his superiors, he would have started World War Three. Instead, Petrov correctly recognized that it was unlikely for the US to attack by launching only five missiles, as he was seeing. It was a bug. Petrov reported it as a computer malfunction, instead of an attack. He was right. World War Three was avoided thanks to a critical judgment of a man  missile offence as described by sensor in place.

Now, let's make a thought experiment. A few years in the future, say 2020, the situation between North Korea and the US is still tense. CNN receives a video from an anonymous source. Kim Jong-un appears in it, discussing with his generals in a locked-up room. It’s not something that was published before.

<img src="/assets/posts/2018-01-01-fake/kim-jong-un.jpg" width="110%">

Korean interpreters are called. The Supreme Leader is ordering the launch of a nuclear missile strike on the imminent [Day of the Sun](https://en.wikipedia.org/wiki/Public_holidays_in_North_Korea). In a matter of hours, the video gets to the Oval Office. The US president must decides to act. He orders a strike. The war starts. But, was the video evidence enough to justify such decision?

<br><br>

**Machine learning in the early 2018.** The issue is that, in a few years time, video content may be fake and at the same time so accurate to be indistinguishable from real ones to human eyes and ears. Facial expressions could be crafted ad-hoc, real people voices could be  labial movements in a video could be adapted to the scripted words . The base video itself might well be a real recording, but the content that it is intended to convey .

While of course video and photo editing have been in use for decades, Artificial Intelligence is lowering the editing effort, the necessary technical expertise and the

<blockquote class="twitter-tweet tw-align-center" data-lang="en"><p lang="en" dir="ltr">4 years of GAN progress (source: <a href="https://t.co/hlxW3NnTJP">https://t.co/hlxW3NnTJP</a> ) <a href="https://t.co/kmK5zikayV">pic.twitter.com/kmK5zikayV</a></p>&mdash; Ian Goodfellow (@goodfellow_ian) <a href="https://twitter.com/goodfellow_ian/status/969776035649675265?ref_src=twsrc%5Etfw">March 3, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


This will change, and *is changing* many industries. Demonstrations of what is to come are popping up from researchers, startups and artists playing with digital media.

Code open source. Computation cheap.

Advertisement, fashion, cinema, tv, design/manufacturing, art.


Think now a bit ahead in a future when potentially every kid -- from their electronic devices -- could generate realistic movies with real people acting in it. This is hardly far fetched. 20 years ago hardly anybody on Earth could have imagined the

 In this situation authorities (government ad well as news outlets) will need a way to diversify their sources of being "real"




  and the use of GAN (Generative Adversarial Networks) will make spotting the edited videos harder and harder.









Machine learning is the technology singularly the most responsible for

Images from caption.
Face swap
Video/audio synchronization
Porn





<br><br>

**A speculative look at potential implications.**

<blockquote class="twitter-tweet tw-align-center" data-lang="en">
<p lang="en" dir="ltr">
The biggest casualty to AI won&#39;t be jobs, but the final and complete eradication of trust in anything you see or hear. <a href="https://t.co/sg9o4v2Q3f">https://t.co/sg9o4v2Q3f</a> <a href="https://t.co/nkj007LtEF">pic.twitter.com/nkj007LtEF</a></p>&mdash; Oli Franklin-Wallis (@olifranklin) <a href="https://twitter.com/olifranklin/status/937660128974852096?ref_src=twsrc%5Etfw">December 4, 2017</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

Information/journalism.

how do you define "authentic"? If a journalist buy a video from a "witness". That could be fake but signed to be true in some way

From Morthen Dahl: True, you'll have to trust the root of a signing chain
So unless the official sender is the root then you only have reputation left
i guess it's similar to the matrix situation where we all live inside a simulation
and the related notion in crypto: unless we have a trusted setup assumption we cannot do anything
https://en.wikipedia.org/wiki/Universal_composability#Impossibility_results

you can see the problem even in the simple case of secure communication through eg asymmetric crypto: how do you know that the public key really belongs to the recipient

at the end of the day, the trust we have in media content at the moment is simply due to the absence of technology that can manipulate it enough to turn its meaning upside down. So basically we believe of something that is photographed or recorded in a video because we all agree in understanding that the content is true since nobody could manipulate it. Would this be the kind of "trust in the setup" ? YES

and you go further... we trust our eye in seeing what's in front of us (not a recording now, the reality in the present) just because we know nobody could manipulate yet
if there was technology for doing that, we would be constantly suspicious as well… NEUROLINK of Muck will destroy also this layer of trust!

Inception. Matrix.


Democracy. Information . This is similar to fake news, but even more serious and pervasive. Even checking the sources (who said that) is not enough. Videos can be fake.

Justice. Digital evidence can be plagiarized at the greatest extent.


<br><br>


**Is the picture really so black?**
Because we are humans, our immune system is ready and it is called “lack of trust”. We will need to look at videos the way Petrov looked at his radar screen If you are an optimist, there is an argument to convince yourself that everything won’t be lost. Once people will be aware that a video recording is not necessarily a trustworthy testimony of facts, we can expect that we will start judging digital media with high suspicion. A video of

This may sounds similar to

Use a bit of imagination to go back in history, and make a comparison with when printing was not a commodity, but instead a privilege of authorities. Before printing became a commodity, a message printed on a poster/paper/journal was regarded as coming from a reputable source, e.g. the government. Since then, today nobody would say that "if printed, it is true " or not even "official", in a weaker sense. This cultural shift will happen as well for more sophistical media vehicle of information ,e.g. photos and videos

<br><br>

**Digital signatures.** How to certify authorship of a digital content? There is already a solution for that and it is called digital signature. It works this way:

How can this be used for certifying the authenticity of a video?


<br><br>

**Machine learning as a solution: building "truth" detectors from data.**


If you are familiar with adversarial training and GANs, those are definitely your first objections to our argument.

<br><br>

**A weekend experiment.** We are not aware of any research work on the topic, so we decided to run a quick experiment over the weekend. Our objective is to verify whether it is feasible to build a statistical model for  distinguishing between real and manipulated images, when the human eyes and brain would have an hard time.

To make the task a bit more concrete, we work on the Caltech's [Faces '99 dataset](http://www.vision.caltech.edu/html-files/archive.html), which contains in total 450 frontal face images of about 27 people.

<br><br>

**Final words.** One day the president of the United States could, before inserting the nuke codes, rely on a technology to certify whether an intelligence source  war by the hostile country of the case *actually* happened.

We must start thinking about this issue as we look at other fundamental ones, such as climate change. It is inevitable. It will profoundly affect human society. It will destroy our trust in what we see, unless we experience it in first person. We need a plan for rescue.




<br>

This post was sparked from many discussions we had with many people. Huge thanks in particular go to [Morten Dahl](https://mortendahl.github.io) for continuous feedback and exciting chats on the topic.


### References

- [[Bojanowski & Joulin ICML17]](https://arxiv.org/abs/1704.05310) Piotr Bojanowski and Armand Joulin, Unsupervised learning by predicting the noise, ICML17
